{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Sr46iwiMAZ"
      },
      "source": [
        "## Sentiment Analysis using RNN\n",
        "\n",
        "Text Classification Task: Sentiment Analysis\n",
        "\n",
        "- Data Preparation: Today we use a dataset (IMDb) for movie reviews. Each review is labeled as either positive or negative.\n",
        "\n",
        "- Preprocessing: Tokenize the text and convert words to integers. Pad sequences to ensure they have the same length.\n",
        "\n",
        "- Model Definition:\n",
        "using an RNN layer to capture the sequential nature of the reviews.\n",
        "Add a Dense layer with a sigmoid activation for binary classification.\n",
        "\n",
        "- Training:\n",
        "Train the model on the training dataset.\n",
        "\n",
        "- Evaluation:\n",
        "Test on a separate validation set and evaluate performance using metrics like accuracy or F1-score.\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "In this, we have to predict the number of positive and negative reviews based on sentiments by using RNN archticture. This is workable example on Many to One type as it takes sentances and output if it's negative or positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnLdD0EYpVYL",
        "outputId": "1761d665-b3dd-48c5-b62f-c1c0d0599393"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Embedding\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import unicodedata\n",
        "import html\n",
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7RGI9_6ySnP",
        "outputId": "c3304191-10bc-4bc3-9658-c6bbed283859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ],
      "source": [
        "raw_data = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "print(raw_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "positive    25000\n",
              "negative    25000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data.sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR5el2yL9adg",
        "outputId": "7b5158f1-436d-4337-ca42-53f62edefdfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 40000\n",
            "Testing set size: 10000\n"
          ]
        }
      ],
      "source": [
        "X = raw_data['review']  # Features: reviews\n",
        "raw_data['label'] = raw_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "y = raw_data['label']  # Labels: sentiment (positive/negative)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "922DHrktiSUM"
      },
      "source": [
        "### Data Prerocessing Pipeline\n",
        "\n",
        "- remove_special_chars(text)\n",
        "\n",
        "Purpose: Clean the input text by removing special characters and HTML entities.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Compile a regex to match multiple spaces.\n",
        "\n",
        "Convert text to lowercase.\n",
        "\n",
        "Replace specific HTML character codes with their corresponding characters.\n",
        "\n",
        "Replace newline characters and HTML tags with appropriate representations.\n",
        "\n",
        "Use html.unescape to convert any remaining HTML entities.\n",
        "\n",
        "Replace multiple spaces with a single space.\n",
        "\n",
        "- remove_non_ascii(text)\n",
        "\n",
        "Purpose: Eliminate non-ASCII characters from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Normalize the text to a compatible Unicode format.\n",
        "\n",
        "Encode to ASCII, ignoring non-ASCII characters.\n",
        "\n",
        "Decode back to UTF-8 format.\n",
        "\n",
        "- to_lowercase(text)\n",
        "\n",
        "Purpose: Convert all characters in the text to lowercase.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Simply return the text converted to lowercase.\n",
        "\n",
        "- remove_punctuation(text)\n",
        "\n",
        "Purpose: Strip punctuation from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create a translation table that maps punctuation characters to None.\n",
        "\n",
        "Use the translation table to translate the text.\n",
        "\n",
        "- replace_numbers(text)\n",
        "\n",
        "Purpose: Remove all integer occurrences from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Use a regex to find and replace all digits with an empty string.\n",
        "\n",
        "- remove_whitespaces(text)\n",
        "\n",
        "Purpose: Trim leading and trailing whitespace from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Return the text after applying the strip() method.\n",
        "\n",
        "- remove_stopwords(words, stop_words)\n",
        "\n",
        "Purpose: Filter out common stopwords from a list of words.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Return a list of words that are not present in the provided stop_words set.\n",
        "\n",
        "- stem_words(words)\n",
        "\n",
        "Purpose: Apply stemming to a list of words.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create an instance of a stemmer.\n",
        "\n",
        "Return a list of stemmed words using the stemmer.\n",
        "\n",
        "- lemmatize_words(words)\n",
        "\n",
        "Purpose: Lemmatize words in the text to their base form.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create an instance of a lemmatizer.\n",
        "\n",
        "Return a list of lemmatized words.\n",
        "\n",
        "- lemmatize_verbs(words)\n",
        "\n",
        "Purpose: Specifically lemmatize verbs in the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create an instance of a lemmatizer.\n",
        "\n",
        "Return a string of lemmatized verbs, maintaining space between words.\n",
        "\n",
        "- text2words(text)\n",
        "\n",
        "Purpose: Tokenize the input text into a list of words.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Use a word tokenizer to split the text into individual words and return the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8jRsBGaZ8MFJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_special_chars(text):\n",
        "    re1 = re.compile(r'  +')\n",
        "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
        "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x1))\n",
        "\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def replace_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def remove_stopwords(words, stop_words):\n",
        "    \"\"\"\n",
        "    :param words:\n",
        "    :type words:\n",
        "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "    or\n",
        "    from spacy.lang.en.stop_words import STOP_WORDS\n",
        "    :type stop_words:\n",
        "    :return:\n",
        "    :rtype:\n",
        "    \"\"\"\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in text\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize words in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
        "\n",
        "def text2words(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def normalize_text( text):\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = replace_numbers(text)\n",
        "    words = text2words(text)\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    # words = stem_words(words)# Either stem ovocar lemmatize\n",
        "    words = lemmatize_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "\n",
        "    return ''.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HZzQakmp8U4P"
      },
      "outputs": [],
      "source": [
        "def normalize_corpus(corpus):\n",
        "  return [normalize_text(t) for t in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "myIUvjmm8bIh"
      },
      "outputs": [],
      "source": [
        "proc_X_train = normalize_corpus(X_train)\n",
        "proc_X_test = normalize_corpus(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVj7vxZziYpm"
      },
      "source": [
        "### Building Pre-trained word embeddings using Glove6B With Bi-Directional LSTM\n",
        "\n",
        "Overview of Layers\n",
        "\n",
        "- Embedding Layer:\n",
        "\n",
        "This layer initializes with the pre-trained GloVe embeddings.\n",
        "\n",
        "You will need to load the GloVe vectors and create an embedding matrix where each word index corresponds to its GloVe vector.\n",
        "\n",
        "- Bidirectional LSTM Layer:\n",
        "\n",
        "This layer processes sequences in both forward and backward directions, capturing context from both sides.\n",
        "It consists of two LSTM layers: one for the forward pass and another for the backward pass.\n",
        "\n",
        "- Dense Layer(s):\n",
        "\n",
        "Typically, you'll have one or more fully connected layers to output your final predictions.\n",
        "The last dense layer often uses a softmax activation for classification tasks.\n",
        "\n",
        "- Output Layer:\n",
        "\n",
        "This layer generates the final predictions, which can be class labels, probabilities, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ebzqLgIhBSK4",
        "outputId": "f1b7c19d-ac55-4b25-927f-3e92f4cad5e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 300)          36892500  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 100, 200)         320800    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100, 200)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 200)              240800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,454,301\n",
            "Trainable params: 561,801\n",
            "Non-trainable params: 36,892,500\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 33s 23ms/step - loss: 0.4339 - acc: 0.8036\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.3420 - acc: 0.8533\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.3029 - acc: 0.8748\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.2720 - acc: 0.8887\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.2344 - acc: 0.9079\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.1928 - acc: 0.9258\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.1537 - acc: 0.9429\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.1176 - acc: 0.9570\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.0872 - acc: 0.9693\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 0.0721 - acc: 0.9745\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.0801 - acc: 0.9709\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.0442 - acc: 0.9851\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.0359 - acc: 0.9883\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.0328 - acc: 0.9888\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.0321 - acc: 0.9889\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 0.0304 - acc: 0.9900\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 0.0276 - acc: 0.9900\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.0262 - acc: 0.9911\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 29s 24ms/step - loss: 0.0244 - acc: 0.9921\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.0222 - acc: 0.9925\n",
            "Test Accuracy: 86.970001\n"
          ]
        }
      ],
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(proc_X_train)  # Fit on training data only\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# Integer encode the training documents\n",
        "encoded_train_docs = t.texts_to_sequences(proc_X_train)\n",
        "# Integer encode the testing documents\n",
        "encoded_test_docs = t.texts_to_sequences(proc_X_test)\n",
        "\n",
        "# Pad documents to a max length of 100 words (adjust as necessary)\n",
        "max_length = 100\n",
        "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
        "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "# Load the whole embedding into memory (make sure to have the GloVe file I added it above when I used stanford api)\n",
        "embeddings_index = dict()\n",
        "with open('glove.6B.300d.txt', mode='rt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define model using LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True)))  # LSTM layer\n",
        "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
        "model.add(Bidirectional(LSTM(100)))  # Another LSTM layer\n",
        "model.add(Dropout(0.5))  # Another Dropout layer\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(padded_train_docs, y_train, epochs=20, verbose=1)  # Reduced epochs for quicker training\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(padded_test_docs, y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (accuracy * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "p1oaSC9b4vM2",
        "outputId": "8e3cc449-561d-433f-af10-30d8e9af24f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 300)          36892500  \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 100)               40100     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,932,701\n",
            "Trainable params: 40,201\n",
            "Non-trainable params: 36,892,500\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.6666 - acc: 0.6019\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.6633 - acc: 0.5870\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6752 - acc: 0.5696\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 0.6861 - acc: 0.5442\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 0.6837 - acc: 0.5515\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 38s 30ms/step - loss: 0.6758 - acc: 0.5643\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 0.6887 - acc: 0.5403\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.6878 - acc: 0.5435\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 0.6793 - acc: 0.5656\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 39s 32ms/step - loss: 0.6488 - acc: 0.6268\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6604 - acc: 0.6047\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.6778 - acc: 0.5713\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6873 - acc: 0.5433\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6864 - acc: 0.5436\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.6783 - acc: 0.5653\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 0.6718 - acc: 0.5837\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 42s 33ms/step - loss: 0.6683 - acc: 0.5892\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.6615 - acc: 0.6026\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.6797 - acc: 0.5616\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6819 - acc: 0.5545\n",
            "Test Accuracy: 56.950003\n"
          ]
        }
      ],
      "source": [
        "# Prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(proc_X_train)  # Fit on training data only\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# Integer encode the training documents\n",
        "encoded_train_docs = t.texts_to_sequences(proc_X_train)\n",
        "# Integer encode the testing documents\n",
        "encoded_test_docs = t.texts_to_sequences(proc_X_test)\n",
        "\n",
        "# Pad documents to a max length of 100 words (adjust as necessary)\n",
        "max_length = 100\n",
        "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
        "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "# Load the whole embedding into memory (make sure to have the GloVe file)\n",
        "embeddings_index = dict()\n",
        "with open('glove.6B.300d.txt', mode='rt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define model using Vanilla RNN\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(SimpleRNN(100, return_sequences=False))  # Simple RNN layer\n",
        "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(padded_train_docs, y_train, epochs=20, verbose=1)  # Reduced epochs for quicker training\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(padded_test_docs, y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "0Olj82_w4rE_",
        "outputId": "4730068c-c796-4ca8-f1be-80b49a2fa411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 100, 300)          36892500  \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 100)               120600    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,013,201\n",
            "Trainable params: 120,701\n",
            "Non-trainable params: 36,892,500\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4430 - acc: 0.7943\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3211 - acc: 0.8665\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.2807 - acc: 0.8835\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.2457 - acc: 0.9001\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.2102 - acc: 0.9167\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.1661 - acc: 0.9376\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.1271 - acc: 0.9542\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0930 - acc: 0.9677\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0724 - acc: 0.9760\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0538 - acc: 0.9825\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0479 - acc: 0.9844\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0438 - acc: 0.9858\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0372 - acc: 0.9880\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.0303 - acc: 0.9903\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0335 - acc: 0.9888\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.0283 - acc: 0.9911\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0303 - acc: 0.9896\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0242 - acc: 0.9925\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 0.0248 - acc: 0.9922\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 0.0234 - acc: 0.9926\n",
            "Test Accuracy: 86.600000\n"
          ]
        }
      ],
      "source": [
        "# Prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(proc_X_train)  # Fit on training data only\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# Integer encode the training documents\n",
        "encoded_train_docs = t.texts_to_sequences(proc_X_train)\n",
        "# Integer encode the testing documents\n",
        "encoded_test_docs = t.texts_to_sequences(proc_X_test)\n",
        "\n",
        "# Pad documents to a max length of 100 words (adjust as necessary)\n",
        "max_length = 100\n",
        "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
        "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "# Load the whole embedding into memory (make sure to have the GloVe file)\n",
        "embeddings_index = dict()\n",
        "with open('glove.6B.300d.txt', mode='rt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define model using GRU\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(GRU(100))  # GRU layer\n",
        "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(padded_train_docs, y_train, epochs=20, batch_size = 32, verbose=1)  # Reduced epochs for quicker training\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(padded_test_docs, y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 100, 300)          36892500  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 100, 200)         320800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 100, 200)          0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 200)              240800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,454,301\n",
            "Trainable params: 561,801\n",
            "Non-trainable params: 36,892,500\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 35s 26ms/step - loss: 0.4197 - accuracy: 0.8106 - val_loss: 0.3750 - val_accuracy: 0.8462\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.3340 - accuracy: 0.8600 - val_loss: 0.3446 - val_accuracy: 0.8551\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.2989 - accuracy: 0.8753 - val_loss: 0.3157 - val_accuracy: 0.8651\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.2640 - accuracy: 0.8932 - val_loss: 0.2942 - val_accuracy: 0.8747\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.2347 - accuracy: 0.9060 - val_loss: 0.3004 - val_accuracy: 0.8805\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.1988 - accuracy: 0.9243 - val_loss: 0.3043 - val_accuracy: 0.8815\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.1594 - accuracy: 0.9403 - val_loss: 0.3298 - val_accuracy: 0.8815\n",
            "Test Accuracy: 87.47%\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from numpy import asarray, zeros\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(proc_X_train)  \n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "encoded_train_docs = t.texts_to_sequences(proc_X_train)\n",
        "encoded_test_docs = t.texts_to_sequences(proc_X_test)\n",
        "\n",
        "max_length = 100\n",
        "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
        "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "embeddings_index = dict()\n",
        "with open('glove.6B.300d.txt', mode='rt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True))) \n",
        "model.add(Dropout(0.6))  \n",
        "model.add(Bidirectional(LSTM(100))) \n",
        "model.add(Dropout(0.6)) \n",
        "model.add(Dense(1, activation='sigmoid'))  \n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(padded_train_docs, y_train, epochs=20, batch_size=32, validation_data=(padded_test_docs, y_test), verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = model.evaluate(padded_test_docs, y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 300)          39677400  \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 100, 200)         320800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 100, 200)          0         \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 200)              240800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,239,201\n",
            "Trainable params: 561,801\n",
            "Non-trainable params: 39,677,400\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 41s 31ms/step - loss: 0.4502 - accuracy: 0.7891 - val_loss: 0.3502 - val_accuracy: 0.8496\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.3514 - accuracy: 0.8515 - val_loss: 0.3139 - val_accuracy: 0.8660\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 34s 28ms/step - loss: 0.3201 - accuracy: 0.8670 - val_loss: 0.2997 - val_accuracy: 0.8734\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.2877 - accuracy: 0.8820 - val_loss: 0.3069 - val_accuracy: 0.8719\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.2604 - accuracy: 0.8939 - val_loss: 0.3053 - val_accuracy: 0.8733\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.2299 - accuracy: 0.9092 - val_loss: 0.2902 - val_accuracy: 0.8792\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.1968 - accuracy: 0.9250 - val_loss: 0.2986 - val_accuracy: 0.8786\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.1639 - accuracy: 0.9394 - val_loss: 0.3516 - val_accuracy: 0.8768\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.1337 - accuracy: 0.9517 - val_loss: 0.3280 - val_accuracy: 0.8801\n",
            "Test Accuracy: 87.92%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: sentiment_analysis_model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: sentiment_analysis_model\\assets\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import html\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GRU, Dropout, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from numpy import asarray, zeros\n",
        "import unicodedata\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "raw_data = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "raw_data['label'] = raw_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "X = raw_data['review']\n",
        "y = raw_data['label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    re1 = re.compile(r'  +')\n",
        "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
        "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x1))\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def replace_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "def remove_stopwords(words, stop_words):\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def text2words(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = replace_numbers(text)\n",
        "    words = text2words(text)\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    words = lemmatize_words(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def normalize_corpus(corpus):\n",
        "    return [normalize_text(t) for t in corpus]\n",
        "\n",
        "proc_X_train = normalize_corpus(X_train)\n",
        "proc_X_test = normalize_corpus(X_test)\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(proc_X_train)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "encoded_train_docs = t.texts_to_sequences(proc_X_train)\n",
        "encoded_test_docs = t.texts_to_sequences(proc_X_test)\n",
        "\n",
        "max_length = 100\n",
        "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
        "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "embeddings_index = dict()\n",
        "with open('glove.6B.300d.txt', mode='rt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True))) \n",
        "model.add(Dropout(0.5)) \n",
        "model.add(Bidirectional(LSTM(100))) \n",
        "model.add(Dropout(0.5)) \n",
        "model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(padded_train_docs, y_train, epochs=20, batch_size=32, validation_data=(padded_test_docs, y_test), verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "loss, accuracy = model.evaluate(padded_test_docs, y_test, verbose=0)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "model.save('sentiment_analysis_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(t, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TF",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
